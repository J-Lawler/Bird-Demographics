
@article{vehtari_rank-normalization_2021,
	title = {Rank-{Normalization}, {Folding}, and {Localization}: {An} {Improved} {Rˆ} for {Assessing} {Convergence} of {MCMC} (with {Discussion})},
	volume = {16},
	issn = {1936-0975, 1931-6690},
	shorttitle = {Rank-{Normalization}, {Folding}, and {Localization}},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full},
	doi = {10.1214/20-BA1221},
	abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic Rˆ of Gelman and Rubin (1992) has serious flaws. Traditional Rˆ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
	number = {2},
	urldate = {2023-06-30},
	journal = {Bayesian Analysis},
	author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
	month = jun,
	year = {2021},
	note = {Publisher: International Society for Bayesian Analysis},
	pages = {667--718},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\PUKG9FGL\\Vehtari et al. - 2021 - Rank-Normalization, Folding, and Localization An .pdf:application/pdf},
}

@article{lahoz-monfort_capturerecapture_2011,
	title = {A capture–recapture model for exploring multi-species synchrony in survival},
	volume = {2},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2010.00050.x},
	doi = {10.1111/j.2041-210X.2010.00050.x},
	abstract = {1. Although recent decades have seen much development of statistical methods to estimate demographical parameters such as reproduction, and survival and migration probabilities, the focus is usually the estimation of parameters for individual species. This is despite the fact that several species may live in close proximity, sometimes competing for the same resources. There is therefore a great need for new methods that enable a better integration of demographical data, e.g. the study of synchrony between sympatric species, which are subject to common environmental stochasticity and potentially similar biotic interactions. 2. We propose a mark–recapture statistical model that uses random effect terms for studying synchrony in a demographical parameter at a multi-species level, adapting a framework initially developed to study multi-site synchrony to this novel situation. The model allows us to divide between-year variance in a demographical parameter into a ‘synchronous’ component, common to all species considered, and species-specific ‘asynchronous’ components, as well as to estimate the proportion of each component accounted for by environmental covariates. 3. We demonstrate the method with data from three colonially breeding auk species that share resources during the breeding season at the Isle of May, Scotland. Mark-resight information has been collected since 1984 for Atlantic puffins Fratercula arctica, common guillemots Uria aalge and razorbills Alca torda marked as breeding adults. We explore the relationship between synchrony in the species’ survival and two environmental covariates. 4. Most of the between-year variation was synchronous to the three species, and the same environmental covariates acted simultaneously as synchronising and desynchronising agents of adult survival, possibly through different indirect causation paths. 5. Synthesis and applications. The model proposed allows the investigation of multi-species synchrony and asynchrony in adult survival, as well as the role of environmental covariates in generating them. It provides insight into whether sympatric species respond similarly or differently to changes in their environment, and helps to disentangle the sources of these differences. The estimated indices of synchrony/asynchrony can facilitate the generation of further hypotheses about similarities/differences in these species’ ecology, such as the potential overlap of wintering areas. The method is readily applicable to other species, ecosystems and demographical parameters.},
	language = {en},
	number = {1},
	urldate = {2023-07-03},
	journal = {Methods in Ecology and Evolution},
	author = {Lahoz-Monfort, José J. and Morgan, Byron J. T. and Harris, Michael P. and Wanless, Sarah and Freeman, Stephen N.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2010.00050.x},
	keywords = {adult survival, Atlantic puffin, Bayesian models, common guillemot, environmental covariates, interspecific synchronisation, partition of variance, random effects, razorbill, WinBUGS},
	pages = {116--124},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\WW2IH5RI\\Lahoz-Monfort et al. - 2011 - A capture–recapture model for exploring multi-spec.pdf:application/pdf;Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\FYU5BUQ6\\j.2041-210X.2010.00050.html:text/html},
}

@article{king_statistical_2014,
	title = {Statistical {Ecology}},
	volume = {1},
	url = {https://doi.org/10.1146/annurev-statistics-022513-115633},
	doi = {10.1146/annurev-statistics-022513-115633},
	abstract = {Statistical ecology deals with the development of new methodologies for analyzing ecological data. Advanced statistical models and techniques are often needed to provide robust analyses of the available data. The statistical models that are developed can often be separated into two distinct processes: a system process that describes the underlying biological system and an observation process that describes the data collection process. The system process is often a function of the demographic parameters of interest, such as survival probabilities, transition rates between states, and/or abundance, whereas the model parameters associated with the observation process are conditional on the underlying state of the system. This review focuses on a number of common forms of ecological data and discusses their associated models and model-fitting approaches, including the incorporation of heterogeneity within the given biological system and the integration of different data sources.},
	number = {1},
	urldate = {2023-07-03},
	journal = {Annual Review of Statistics and Its Application},
	author = {King, Ruth},
	year = {2014},
	note = {\_eprint: https://doi.org/10.1146/annurev-statistics-022513-115633},
	keywords = {heterogeneity, integrated data, Markov model, observation process, system process},
	pages = {401--426},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\CA92VDPC\\King - 2014 - Statistical Ecology.pdf:application/pdf},
}

@book{morgan_analysis_2014,
	address = {New York},
	title = {Analysis of {Capture}-{Recapture} {Data}},
	isbn = {978-0-429-09384-5},
	abstract = {An important first step in studying the demography of wild animals is to identify the animals uniquely through applying markings, such as rings, tags, and bands. Once the animals are encountered again, researchers can study different forms of capture-recapture data to estimate features, such as the mortality and size of the populations. Capture-rec},
	publisher = {Chapman and Hall/CRC},
	author = {Morgan, Byron J. T., Rachel S. McCrea},
	month = jul,
	year = {2014},
	doi = {10.1201/b17222},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\L8H5TIRV\\Morgan - 2014 - Analysis of Capture-Recapture Data.pdf:application/pdf},
}

@incollection{king_chapter_2019,
	series = {Integrated {Population} {Biology} and {Modeling}, {Part} {B}},
	title = {Chapter 2 - {Capture}–{Recapture} {Methods} and {Models}: {Estimating} {Population} {Size}},
	volume = {40},
	shorttitle = {Chapter 2 - {Capture}–{Recapture} {Methods} and {Models}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169716118300877},
	abstract = {This book chapter describes ecological capture–recapture studies and associated models often fitted to capture–recapture data to obtain estimates of total population size. Such estimates can be important for numerous reasons, including, for example, conservation and management purposes. We focus on different forms of heterogeneity that may affect the propensity of individuals to be observed within the study period. Failing to account for such heterogeneity can lead to significant bias in the population estimates. We focus on different types of heterogeneity corresponding to recorded (discrete-valued) covariates/characteristics of individuals that are observed within the study period; in addition to unobserved heterogeneity in the form of mixture distributions. The different models are motivated and discussed, including the specification of the likelihood functions, before being applied to a real dataset. Finally we conclude with a discussion including the modern challenges which are arising due to technological advances.},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {King, Ruth and McCrea, Rachel},
	editor = {Srinivasa Rao, Arni S. R. and Rao, C. R.},
	month = jan,
	year = {2019},
	doi = {10.1016/bs.host.2018.09.006},
	keywords = {Abundance, Capture histories, Discrete covariates, Heterogeneity, Likelihood, Mixtures, Models},
	pages = {33--83},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\WTJG2IF3\\King and McCrea - 2019 - Chapter 2 - Capture–Recapture Methods and Models .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\PHTMCQ5Z\\S0169716118300877.html:text/html},
}

@article{cuadrado_all_1995,
	title = {Do all {Blackcaps} {Sylvia} atricapilla show winter site fidelity?},
	volume = {137},
	issn = {1474-919X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1474-919X.1995.tb03221.x},
	doi = {10.1111/j.1474-919X.1995.tb03221.x},
	abstract = {Fidelity to previous wintering areas (i.e. site fidelity) has important ecological and evolutionary implications. However, since the percentage of recaptures of ringed birds at the same wintering area in subsequent years does not allow the estimation of the proportion of birds alive that exhibit site fidelity, previous studies on different passerine species have failed to show the true extent of this site fidelity. Here we use a recent approach, based on the comparison of survival rate estimates from capture-recapture data at single field stations with recovery data from a much larger area. The idea is to determine the proportion of birds still alive that return to the area. The study was carried out on the Blackcap Sylvia atricapilla, comparing the capture-recapture data of two field stations (Pilas [SW] and Tiana [NE]) situated 1000 km apart in Spain with the complete winter recovery data of the Spanish Ringing Office. Totals of 1936 and 3976 Blackcaps were ringed at Pilas (1981–1986) and Tiana (1975–1986), respectively, and the numbers of recaptures in subsequent winters were 94 and 34. The pooled annual survival rate estimation based on the two sites is 0.40 (s.e. = 0.05). The annual survival rate estimated from the Spanish ringing recovery data was 0.48 (s.e. = 0.08), which is not significantly different. We can conclude, therefore, that migrant Blackcaps in Spain, if alive, tend to return year after year to the previous wintering site.},
	language = {en},
	number = {1},
	urldate = {2023-07-04},
	journal = {Ibis},
	author = {Cuadrado, Mariano and Senar, Juan Carlos and Copete, Jose Luis},
	year = {1995},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1474-919X.1995.tb03221.x},
	pages = {70--75},
	file = {Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\HM4B5YJX\\j.1474-919X.1995.tb03221.html:text/html},
}

@article{gimenez_individual_2010,
	title = {Individual heterogeneity in studies on marked animals using numerical integration: capture–recapture mixed models},
	volume = {91},
	copyright = {© 2010 by the Ecological Society of America},
	issn = {1939-9170},
	shorttitle = {Individual heterogeneity in studies on marked animals using numerical integration},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1890/09-1903.1},
	doi = {10.1890/09-1903.1},
	abstract = {In conservation and evolutionary ecology, quantifying and accounting for individual heterogeneity in vital rates of open populations is of particular interest. Individual random effects have been used in capture–recapture models, adopting a Bayesian framework with Markov chain Monte Carlo (MCMC) to carry out estimation and inference. As an alternative, we show how numerical integration via the Gauss-Hermite quadrature (GHQ) can be efficiently used to approximate the capture–recapture model likelihood with individual random effects. We compare the performance of the two approaches (MCMC vs. GHQ) and finite mixture models using two examples, including data on European Dippers and Sociable Weavers. Besides relying on standard statistical tools, GHQ was found to be faster than MCMC simulations. Our approach is implemented in program E-SURGE. Overall, capture–recapture mixed models (CR2Ms), implemented either via a GHQ approximation or MCMC simulations, have potential important applications in population biology.},
	language = {en},
	number = {4},
	urldate = {2023-07-04},
	journal = {Ecology},
	author = {Gimenez, O. and Choquet, R.},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1890/09-1903.1},
	keywords = {random effects, WinBUGS, capture–recapture mixed models, CR2M, European Dippers, finite mixture models, generalized linear mixed models, likelihood-ratio test, mark–recapture models, Sociable Weavers, survival estimation},
	pages = {951--957},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\VFHCUAID\\Gimenez and Choquet - 2010 - Individual heterogeneity in studies on marked anim.pdf:application/pdf;Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\DSPUSPAZ\\09-1903.html:text/html},
}

@book{borchers_estimating_2013,
	title = {Estimating {Animal} {Abundance}: {Closed} {Populations}},
	isbn = {978-1-4471-3708-5},
	shorttitle = {Estimating {Animal} {Abundance}},
	abstract = {We hope this book will make the bewildering variety of methods for estimat ing the abundance of animal populations more accessible to the uninitiated and more coherent to the cogniscenti. We have tried to emphasize the fun damental similarity of many methods and to draw out the common threads that underlie them. With the exception of Chapter 13, we restrict ourselves to closed populations (those that do not change in composition over the period(s) being considered). Open population methods are in many ways simply extensions of closed population methods, and we have tried to pro vide the reader with a foundation on which understanding of both closed and open population methods can develop. We would like to thank Miguel Bernal for providing the St Andrews example dataset used frequently in the book; Miguel Bernal and Jeff Laake for commenting on drafts of the book; Jeff Laake for providing Figure 10.1; NRC Research Press for allowing us to use Figures 10.2, 10.3, 10.4, 10.5, 10.6 and 10.7; the International Whaling Commission for allowing us to use Figure 12.1; Sharon Hedley for providing Figures 12.1 and 12.2. D.L.B. is eternally indebted to Carol, Alice and Aidan for their support through writing the book, and for the many evenings and weekends that it has taken from them.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Borchers, D. L. and Buckland, Stephen T. and Zucchini, Walter},
	month = mar,
	year = {2013},
	note = {Google-Books-ID: bf7ZBwAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Applied, Mathematics / General, Medical / Biostatistics, Science / Life Sciences / Ecology, Science / Life Sciences / Zoology / General},
}

@article{king_bayesian_2008,
	title = {On the {Bayesian} {Estimation} of a {Closed} {Population} {Size} in the {Presence} of {Heterogeneity} and {Model} {Uncertainty}},
	volume = {64},
	copyright = {© 2008, The International Biometric Society},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00938.x},
	doi = {10.1111/j.1541-0420.2007.00938.x},
	abstract = {We consider the estimation of the size of a closed population, often of interest for wild animal populations, using a capture–recapture study. The estimate of the total population size can be very sensitive to the choice of model used to fit to the data. We consider a Bayesian approach, in which we consider all eight plausible models initially described by Otis et al. (1978, Wildlife Monographs62, 1–135) within a single framework, including models containing an individual heterogeneity component. We show how we are able to obtain a model-averaged estimate of the total population, incorporating both parameter and model uncertainty. To illustrate the methodology we initially perform a simulation study and analyze two datasets where the population size is known, before considering a real example relating to a population of dolphins off northeast Scotland.},
	language = {en},
	number = {3},
	urldate = {2023-07-04},
	journal = {Biometrics},
	author = {King, R. and Brooks, S. P.},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2007.00938.x},
	keywords = {Heterogeneity, Bayesian approach, Model-averaging, Population size, Reversible jump Markov chain Monte Carlo},
	pages = {816--824},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\8CN53MHE\\King and Brooks - 2008 - On the Bayesian Estimation of a Closed Population .pdf:application/pdf;Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\5LTRGJB6\\j.1541-0420.2007.00938.html:text/html},
}

@article{otis_statistical_1978,
	title = {Statistical {Inference} from {Capture} {Data} on {Closed} {Animal} {Populations}},
	issn = {0084-0173},
	url = {https://www.jstor.org/stable/3830650},
	number = {62},
	urldate = {2023-07-04},
	journal = {Wildlife Monographs},
	author = {Otis, David L. and Burnham, Kenneth P. and White, Gary C. and Anderson, David R.},
	year = {1978},
	note = {Publisher: [Wiley, Wildlife Society]},
	pages = {3--135},
}

@incollection{pledger_stopover_2009,
	address = {Boston, MA},
	series = {Environmental and {Ecological} {Statistics}},
	title = {Stopover {Duration} {Analysis} with {Departure} {Probability} {Dependent} on {Unknown} {Time} {Since} {Arrival}},
	isbn = {978-0-387-78151-8},
	url = {https://doi.org/10.1007/978-0-387-78151-8_15},
	abstract = {In stopover duration analysis for migratory birds, models with the probability of departure dependent upon time since arrival are useful if the birds are stopping over to replenish body fat. In capture–recapture studies, the exact time of arrival is not generally known, as a bird may not be captured soon after arrival, or it may not be captured at all. We present models which allow for the uncertain knowledge of arrival time, while providing estimates of the total number of birds stopping over, and the distribution and mean of true stopover times for the population.},
	language = {en},
	urldate = {2023-07-04},
	booktitle = {Modeling {Demographic} {Processes} {In} {Marked} {Populations}},
	publisher = {Springer US},
	author = {Pledger, Shirley and Efford, Murray and Pollock, Kenneth and Collazo, Jaime and Lyons, James},
	editor = {Thomson, David L. and Cooch, Evan G. and Conroy, Michael J.},
	year = {2009},
	doi = {10.1007/978-0-387-78151-8_15},
	keywords = {Age-related survival, Capture–recapture, Jolly–Seber model, Mark-recapture, migratory birds, residence time, Schwarz–Arnason model, Stopover duration, Survival curve},
	pages = {349--363},
}

@article{pledger_unified_2000,
	title = {Unified {Maximum} {Likelihood} {Estimates} for {Closed} {Capture}–{Recapture} {Models} {Using} {Mixtures}},
	volume = {56},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2000.00434.x},
	doi = {10.1111/j.0006-341X.2000.00434.x},
	abstract = {Summary. Agresti (1994, Biometrics50, 494–500) and Norris and Pollock (1996a, Biometrics52, 639–649) suggested using methods of finite mixtures to partition the animals in a closed capture-recapture experiment into two or more groups with relatively homogeneous capture probabilities. This enabled them to fit the models Mh, Mbh (Norris and Pollock), and Mth (Agresti) of Otis et al. (1978, Wildlife Monographs62, 1–135). In this article, finite mixture partitions of animals and/or samples are used to give a unified linear-logistic framework for fitting all eight models of Otis et al. by maximum likelihood. Likelihood ratio tests are available for model comparisons. For many data sets, a simple dichotomy of animals is enough to substantially correct for heterogeneity-induced bias in the estimation of population size, although there is the option of fitting more than two groups if the data warrant it.},
	language = {en},
	number = {2},
	urldate = {2023-07-04},
	journal = {Biometrics},
	author = {Pledger, Shirley},
	year = {2000},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2000.00434.x},
	keywords = {Heterogeneity, Capture–recapture, Boundary estimation, Closed populations, Maximum likelihood, Mixture distribution, Multinomial model},
	pages = {434--442},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\W969EGG4\\Pledger - 2000 - Unified Maximum Likelihood Estimates for Closed Ca.pdf:application/pdf;Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\GJ33HK2A\\j.0006-341X.2000.00434.html:text/html},
}

@article{pradel_capture-recapture_1997,
	title = {Capture-{Recapture} {Survival} {Models} {Taking} {Account} of {Transients}},
	volume = {53},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2533097},
	doi = {10.2307/2533097},
	abstract = {The presence of transient animals, common enough in natural populations, invalidates the estimation of survival by traditional capture-recapture (CR) models designed for the study of residents only. Also, the study of transit is interesting in itself. We thus develop here a class of CR models to describe the presence of transients. In order to assess the merits of this approach we examine the bias of the traditional survival estimators in the presence of transients in relation to the power of different tests for detecting transients. We also compare the relative efficiency of an ad hoc approach to dealing with transients that leaves out the first observation of each animal. We then study a real example using lazuli bunting (Passerina amoena) and, in conclusion, discuss the design of an experiment aiming at the estimation of transience. In practice, the presence of transients is easily detected whenever the risk of bias is high. The ad hoc approach, which yields unbiased estimates for residents only, is satisfactory in a time-dependent context but poorly efficient when parameters are constant. The example shows that intermediate situations between strict `residence' and strict `transience' may exist in certain studies. Yet, most of the time, if the study design takes into account the expected length of stay of a transient, it should be possible to efficiently separate the two categories of animals.},
	number = {1},
	urldate = {2023-07-04},
	journal = {Biometrics},
	author = {Pradel, Roger and Hines, James E. and Lebreton, Jean-Dominique and Nichols, James D.},
	year = {1997},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {60--72},
	file = {JSTOR Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\8JE58WCD\\Pradel et al. - 1997 - Capture-Recapture Survival Models Taking Account o.pdf:application/pdf},
}

@book{seber_capture-recapture_2019,
	address = {Cham},
	series = {Statistics for {Biology} and {Health}},
	title = {Capture-{Recapture}: {Parameter} {Estimation} for {Open} {Animal} {Populations}},
	isbn = {978-3-030-18186-4 978-3-030-18187-1},
	shorttitle = {Capture-{Recapture}},
	url = {http://link.springer.com/10.1007/978-3-030-18187-1},
	language = {en},
	urldate = {2023-07-04},
	publisher = {Springer International Publishing},
	author = {Seber, George A. F. and Schofield, Matthew R.},
	year = {2019},
	doi = {10.1007/978-3-030-18187-1},
	keywords = {Bayesian models, Acoustic tags, Animal migration, Capture-mark-recapture, Cormack-Jolly –Seber models, Genetic markers, GPS, Monte Carlo Recapture Methods, Ring recovery data, State-space models, Survival estimation, Time-series models},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\Y8D8Z8SH\\Seber and Schofield - 2019 - Capture-Recapture Parameter Estimation for Open A.pdf:application/pdf},
}

@article{telleria_habitat_2007,
	title = {Habitat effects on resource tracking ability: do wintering {Blackcaps} {Sylvia} atricapilla track fruit availability?},
	volume = {149},
	issn = {1474-919X},
	shorttitle = {Habitat effects on resource tracking ability},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1474-919X.2006.00590.x},
	doi = {10.1111/j.1474-919X.2006.00590.x},
	abstract = {If resource availability shapes population distribution, changes in resource abundance should cause parallel changes in population numbers. However, tracking ability may be disrupted by different environmental and behavioural factors that act at different spatial and temporal scales. Here we analyse the ability of wintering Blackcap Sylvia atricapilla populations to track spatio-temporal variation in fruit availability in southern Spain in two habitats (forests and shrublands) with different population structure. Former studies had shown that forests are equally used by both adult migrant and local Blackcaps, whereas shrublands are nearly monopolized by juvenile migrants. These differences might affect resource tracking: it should be disrupted in forests, as local birds remain over winter in their breeding territories, but not in shrublands where similarly competitive juvenile migrants can freely track the spatial distribution of fruits. We analysed the fruit-tracking ability of Blackcap populations among sites and years in both habitat types using a habitat-matching model, which predicts spatio-temporal changes in population abundance proportional to changes in resource availability. We counted Blackcaps and fruiting shrubs (dominated by Lentiscs Pistacia lentiscus and Wild Olives Olea europaea sylvestris) during four winters in forest and shrubland patches. The abundance of fruits was always higher in shrublands than in forests. In shrublands, Blackcaps seemed to move freely across fruit-rich habitat patches, tracking changes in fruiting-shrub abundance among sites and years. However, such tracking was not observed in forests. This supports the view that fruit-tracking ability may be constrained by local factors, such as the social structure of populations occurring in different habitat types, which introduces spatio-temporal variation in the way fruit availability shapes the abundance distribution of these birds in their Mediterranean wintering grounds.},
	language = {en},
	number = {1},
	urldate = {2023-07-04},
	journal = {Ibis},
	author = {Tellería, Jose Luis and Pérez-Tris, Javier},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1474-919X.2006.00590.x},
	pages = {18--25},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\6D9WPDL6\\℡lería and Pérez-Tris - 2007 - Habitat effects on resource tracking ability do w.pdf:application/pdf;Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\KBWY4J6Y\\j.1474-919X.2006.00590.html:text/html},
}

@incollection{white_evaluation_2009,
	address = {Boston, MA},
	series = {Environmental and {Ecological} {Statistics}},
	title = {Evaluation of a {Bayesian} {MCMC} {Random} {Effects} {Inference} {Methodology} for {Capture}-{Mark}-{Recapture} {Data}},
	isbn = {978-0-387-78151-8},
	url = {https://doi.org/10.1007/978-0-387-78151-8_53},
	abstract = {Monte Carlo simulation was used to evaluate properties of a simple Bayesian MCMC analysis of the random effects model for single group Cormack-Jolly-Seber capture-recapture data. The MCMC method is applied to the model via a logit link, so parameters \$p,{\textbackslash} S\$are on a logit scale, where \${\textbackslash}mathrm\{logit\}(S)\$is assumed to have, and is generated from, a normal distribution with mean \${\textbackslash}upmu\$and variance \${\textbackslash}upsigma{\textasciicircum}\{2\}\$. Marginal prior distributions on \${\textbackslash}mathrm\{logit\}(p)\$and \${\textbackslash}upmu\$were independent normal with mean zero and standard deviation 1.75 for \${\textbackslash}mathrm\{logit\}(p)\$and 100 for \${\textbackslash}upmu\$; hence minimally informative. Marginal prior distribution on \${\textbackslash}upsigma{\textasciicircum}\{2\}\$was placed on \${\textbackslash}uptau{\textasciicircum}\{2\}=1/{\textbackslash}upsigma{\textasciicircum}\{2\}\$as a gamma distribution with \${\textbackslash}upalpha={\textbackslash}upbeta=0.001\$. The study design has 432 points spread over 5 factors: occasions \$(t)\$, new releases per occasion \$(u),{\textbackslash} p,{\textbackslash} {\textbackslash}upmu\$, and \${\textbackslash}upsigma\$. At each design point 100 independent trials were completed (hence 43,200 trials in total), each with sample size \$n=10,000\$from the parameter posterior distribution. At 128 of these design points comparisons are made to previously reported results from a method of moments procedure. We looked at properties of point and interval inference on \${\textbackslash}upmu\$, and \${\textbackslash}upsigma\$based on the posterior mean, median, and mode and equal-tailed 95\% credibility interval. Bayesian inference did very well for the parameter \${\textbackslash}upmu\$, but under the conditions used here, MCMC inference performance for \${\textbackslash}upsigma\$was mixed: poor for sparse data (i.e., only 7 occasions) or \${\textbackslash}upsigma=0\$, but good when there were sufficient data and not small \${\textbackslash}upsigma\$.},
	language = {en},
	urldate = {2023-07-04},
	booktitle = {Modeling {Demographic} {Processes} {In} {Marked} {Populations}},
	publisher = {Springer US},
	author = {White, Gary C. and Burnham, Kenneth P. and Barker, Richard J.},
	editor = {Thomson, David L. and Cooch, Evan G. and Conroy, Michael J.},
	year = {2009},
	doi = {10.1007/978-0-387-78151-8_53},
	keywords = {Bayesian estimation, MCMC, Process covariance, Process variance, Program MARK, Random effects, Variance components},
	pages = {1119--1127},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\T9MHXUUK\\White et al. - 2009 - Evaluation of a Bayesian MCMC Random Effects Infer.pdf:application/pdf},
}

@article{yackulic_need_2020,
	title = {A need for speed in {Bayesian} population models: a practical guide to marginalizing and recovering discrete latent states},
	volume = {30},
	copyright = {Published 2020. This article is a U.S. Government work and is in the public domain in the USA.},
	issn = {1939-5582},
	shorttitle = {A need for speed in {Bayesian} population models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/eap.2112},
	doi = {10.1002/eap.2112},
	abstract = {Bayesian population models can be exceedingly slow due, in part, to the choice to simulate discrete latent states. Here, we discuss an alternative approach to discrete latent states, marginalization, that forms the basis of maximum likelihood population models and is much faster. Our manuscript has two goals: (1) to introduce readers unfamiliar with marginalization to the concept and provide worked examples and (2) to address topics associated with marginalization that have not been previously synthesized and are relevant to both Bayesian and maximum likelihood models. We begin by explaining marginalization using a Cormack-Jolly-Seber model. Next, we apply marginalization to multistate capture–recapture, community occupancy, and integrated population models and briefly discuss random effects, priors, and pseudo-R2. Then, we focus on recovery of discrete latent states, defining different types of conditional probabilities and showing how quantities such as population abundance or species richness can be estimated in marginalized code. Last, we show that occupancy and site-abundance models with auto-covariates can be fit with marginalized code with minimal impact on parameter estimates. Marginalized code was anywhere from five to {\textgreater}1,000 times faster than discrete code and differences in inferences were minimal. Discrete latent states and fully conditional approaches provide the best estimates of conditional probabilities for a given site or individual. However, estimates for parameters and derived quantities such as species richness and abundance are minimally affected by marginalization. In the case of abundance, marginalized code is both quicker and has lower bias than an N-augmentation approach. Understanding how marginalization works shrinks the divide between Bayesian and maximum likelihood approaches to population models. Some models that have only been presented in a Bayesian framework can easily be fit in maximum likelihood. On the other hand, factors such as informative priors, random effects, or pseudo-R2 values may motivate a Bayesian approach in some applications. An understanding of marginalization allows users to minimize the speed that is sacrificed when switching from a maximum likelihood approach. Widespread application of marginalization in Bayesian population models will facilitate more thorough simulation studies, comparisons of alternative model structures, and faster learning.},
	language = {en},
	number = {5},
	urldate = {2023-07-05},
	journal = {Ecological Applications},
	author = {Yackulic, Charles B. and Dodrill, Michael and Dzul, Maria and Sanderlin, Jamie S. and Reid, Janice A.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/eap.2112},
	keywords = {augmentation, autologistic, closed conditional, density dependence, forward conditional, fully conditional, hidden Markov model, mark–recapture, N-occupancy, unconditional},
	pages = {e02112},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\8RJUR8NF\\Yackulic et al. - 2020 - A need for speed in Bayesian population models a .pdf:application/pdf;Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\CQDLCB2V\\eap.html:text/html},
}

@article{yackulic_quantitative_2014,
	title = {A quantitative life history of endangered humpback chub that spawn in the {Little} {Colorado} {River}: variation in movement, growth, and survival},
	volume = {4},
	issn = {2045-7758},
	shorttitle = {A quantitative life history of endangered humpback chub that spawn in the {Little} {Colorado} {River}},
	doi = {10.1002/ece3.990},
	abstract = {While the ecology and evolution of partial migratory systems (defined broadly to include skip spawning) have been well studied, we are only beginning to understand how partial migratory populations are responding to ongoing environmental change. Environmental change can lead to differences in the fitness of residents and migrants, which could eventually lead to changes in the frequency of the strategies in the overall population. Here, we address questions concerning the life history of the endangered Gila cypha (humpback chub) in the regulated Colorado River and the unregulated tributary and primary spawning area, the Little Colorado River. We develop eight multistate models for the population based on three movement hypotheses, in which states are defined in terms of fish size classes and river locations. We fit these models to mark-recapture data collected in 2009-2012. We compare survival and growth estimates between the Colorado River and Little Colorado River and calculate abundances for all size classes. The best model supports the hypotheses that larger adults spawn more frequently than smaller adults, that there are residents in the spawning grounds, and that juveniles move out of the Little Colorado River in large numbers during the monsoon season (July-September). Monthly survival rates for G. cypha in the Colorado River are higher than in the Little Colorado River in all size classes; however, growth is slower. While the hypothetical life histories of life-long residents in the Little Colorado River and partial migrants spending most of its time in the Colorado River are very different, they lead to roughly similar fitness expectations when we used expected number of spawns as a proxy. However, more research is needed because our study period covers a period of years when conditions in the Colorado River for G. cypha are likely to have been better than has been typical over the last few decades.},
	language = {eng},
	number = {7},
	journal = {Ecology and Evolution},
	author = {Yackulic, Charles B. and Yard, Michael D. and Korman, Josh and Haverbeke, David R.},
	month = apr,
	year = {2014},
	pmid = {24772278},
	pmcid = {PMC3997317},
	keywords = {Dams, fitness trade-offs, Grand Canyon, hydrology, multistate, partial migration, size-dependent, skip spawning, tributary},
	pages = {1006--1018},
	file = {Full Text:C\:\\Users\\JakeL\\Zotero\\storage\\LAU68SS2\\Yackulic et al. - 2014 - A quantitative life history of endangered humpback.pdf:application/pdf},
}

@article{yackulic_inferring_2018,
	title = {Inferring species interactions through joint mark-recapture analysis},
	volume = {99},
	issn = {0012-9658},
	doi = {10.1002/ecy.2166},
	abstract = {Introduced species are frequently implicated in declines of native species. In many cases, however, evidence linking introduced species to native declines is weak. Failure to make strong inferences regarding the role of introduced species can hamper attempts to predict population viability and delay effective management responses. For many species, mark-recapture analysis is the more rigorous form of demographic analysis. However, to our knowledge, there are no mark-recapture models that allow for joint modeling of interacting species. Here, we introduce a two-species mark-recapture population model in which the vital rates (and capture probabilities) of one species are allowed to vary in response to the abundance of the other species. We use a simulation study to explore bias and choose an approach to model selection. We then use the model to investigate species interactions between endangered humpback chub (Gila cypha) and introduced rainbow trout (Oncorhynchus mykiss) in the Colorado River between 2009 and 2016. In particular, we test hypotheses about how two environmental factors (turbidity and temperature), intraspecific density dependence, and rainbow trout abundance are related to survival, growth, and capture of juvenile humpback chub. We also project the long-term effects of different rainbow trout abundances on adult humpback chub abundances. Our simulation study suggests this approach has minimal bias under potentially challenging circumstances (i.e., low capture probabilities) that characterized our application and that model selection using indicator variables could reliably identify the true generating model even when process error was high. When the model was applied to rainbow trout and humpback chub, we identified negative relationships between rainbow trout abundance and the survival, growth, and capture probability of juvenile humpback chub. Effects on interspecific interactions on survival and capture probability were strongly supported, whereas support for the growth effect was weaker. Environmental factors were also identified to be important and in many cases stronger than interspecific interactions, and there was still substantial unexplained variation in growth and survival rates. The general approach presented here for combining mark-recapture data for two species is applicable in many other systems and could be modified to model abundance of the invader via other modeling approaches.},
	language = {eng},
	number = {4},
	journal = {Ecology},
	author = {Yackulic, Charles B. and Korman, Josh and Yard, Michael D. and Dzul, Maria},
	month = apr,
	year = {2018},
	pmid = {29465780},
	keywords = {Demography, density dependence, Animals, Bayesian, competition, Gila cypha, interspecific, intraspecific, mark-recapture, Oncorhynchus mykiss, predation, Temperature},
	pages = {812--821},
}

@article{welsh_fitting_2013,
	title = {Fitting and {Interpreting} {Occupancy} {Models}},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0052015},
	doi = {10.1371/journal.pone.0052015},
	abstract = {We show that occupancy models are more difficult to fit than is generally appreciated because the estimating equations often have multiple solutions, including boundary estimates which produce fitted probabilities of zero or one. The estimates are unstable when the data are sparse, making them difficult to interpret, and, even in ideal situations, highly variable. As a consequence, making accurate inference is difficult. When abundance varies over sites (which is the general rule in ecology because we expect spatial variance in abundance) and detection depends on abundance, the standard analysis suffers bias (attenuation in detection, biased estimates of occupancy and potentially finding misleading relationships between occupancy and other covariates), asymmetric sampling distributions, and slow convergence of the sampling distributions to normality. The key result of this paper is that the biases are of similar magnitude to those obtained when we ignore non-detection entirely. The fact that abundance is subject to detection error and hence is not directly observable, means that we cannot tell when bias is present (or, equivalently, how large it is) and we cannot adjust for it. This implies that we cannot tell which fit is better: the fit from the occupancy model or the fit ignoring the possibility of detection error. Therefore trying to adjust occupancy models for non-detection can be as misleading as ignoring non-detection completely. Ignoring non-detection can actually be better than trying to adjust for it.},
	language = {en},
	number = {1},
	urldate = {2023-07-06},
	journal = {PLOS ONE},
	author = {Welsh, Alan H. and Lindenmayer, David B. and Donnelly, Christine F.},
	month = jan,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Bird song, Birds, Curve fitting, Pines, Probability distribution, Simulation and modeling, Statistical models, Theoretical ecology},
	pages = {e52015},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\9BFCLUDA\\Welsh et al. - 2013 - Fitting and Interpreting Occupancy Models.pdf:application/pdf},
}

@book{r_core_team_r_2022,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2022},
}

@book{richardson_arrow_2023,
	title = {arrow: {Integration} to '{Apache}' '{Arrow}'},
	url = {https://CRAN.R-project.org/package=arrow},
	author = {Richardson, Neal and Cook, Ian and Crane, Nic and Dunnington, Dewey and François, Romain and Keane, Jonathan and Moldovan-Grünfeld, Dragoș and Ooms, Jeroen and {Apache Arrow}},
	year = {2023},
	annote = {R package version 12.0.0},
}

@book{xie_bookdown_2023,
	title = {bookdown: {Authoring} {Books} and {Technical} {Documents} with {R} {Markdown}},
	url = {https://github.com/rstudio/bookdown},
	author = {Xie, Yihui},
	year = {2023},
	annote = {R package version 0.33},
}

@book{xie_bookdown_2016,
	address = {Boca Raton, Florida},
	title = {bookdown: {Authoring} {Books} and {Technical} {Documents} with {R} {Markdown}},
	url = {https://bookdown.org/yihui/bookdown},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui},
	year = {2016},
	annote = {ISBN 978-1138700109},
}

@misc{vehtari_loo_2022,
	title = {loo: {Efficient} leave-one-out cross-validation and {WAIC} for {Bayesian} models},
	url = {https://mc-stan.org/loo/},
	author = {Vehtari, Aki and Gabry, Jonah and Magnusson, Mans and Yao, Yuling and Bürkner, Paul-Christian and Paananen, Topi and Gelman, Andrew},
	year = {2022},
	annote = {R package version 2.5.1},
}

@article{vehtari_practical_2017,
	title = {Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}},
	volume = {27},
	doi = {10.1007/s11222-016-9696-4},
	number = {5},
	journal = {Statistics and Computing},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	year = {2017},
	pages = {1413--1432},
}

@article{yao_using_2017,
	title = {Using stacking to average {Bayesian} predictive distributions},
	doi = {10.1214/17-BA1091},
	journal = {Bayesian Analysis},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	year = {2017},
}

@book{r_core_team_r_2022-1,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2022},
}

@book{allaire_rmarkdown_2023,
	title = {rmarkdown: {Dynamic} {Documents} for {R}},
	url = {https://github.com/rstudio/rmarkdown},
	author = {Allaire, J. J. and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
	year = {2023},
	annote = {R package version 2.20},
}

@book{xie_r_2018,
	address = {Boca Raton, Florida},
	title = {R {Markdown}: {The} {Definitive} {Guide}},
	url = {https://bookdown.org/yihui/rmarkdown},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui and Allaire, J. J. and Grolemund, Garrett},
	year = {2018},
	annote = {ISBN 9781138359338},
}

@book{xie_r_2020,
	address = {Boca Raton, Florida},
	title = {R {Markdown} {Cookbook}},
	url = {https://bookdown.org/yihui/rmarkdown-cookbook},
	publisher = {Chapman and Hall/CRC},
	author = {Xie, Yihui and Dervieux, Christophe and Riederer, Emily},
	year = {2020},
	annote = {ISBN 9780367563837},
}

@misc{stan_development_team_rstan_nodate,
	title = {{RStan}: the {R} interface to {Stan}},
	url = {https://mc-stan.org/},
	author = {{Stan Development Team}},
	annote = {R package version 2.26.13},
}

@book{kay_tidybayes_2023,
	title = {tidybayes: {Tidy} {Data} and {Geoms} for {Bayesian} {Models}},
	url = {http://mjskay.github.io/tidybayes/},
	author = {Kay, Matthew},
	year = {2023},
	doi = {10.5281/zenodo.1308151},
	annote = {R package version 3.0.4},
}

@article{wickham_welcome_2019,
	title = {Welcome to the tidyverse},
	volume = {4},
	doi = {10.21105/joss.01686},
	number = {43},
	journal = {Journal of Open Source Software},
	author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and François, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and Müller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
	year = {2019},
	pages = {1686},
}

@article{kowarik_imputation_2016,
	title = {Imputation with the {R} {Package} {VIM}},
	volume = {74},
	doi = {10.18637/jss.v074.i07},
	number = {7},
	journal = {Journal of Statistical Software},
	author = {Kowarik, Alexander and Templ, Matthias},
	year = {2016},
	pages = {1--16},
}

@misc{betancourt_conceptual_2018,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	urldate = {2023-08-06},
	publisher = {arXiv},
	author = {Betancourt, Michael},
	month = jul,
	year = {2018},
	note = {arXiv:1701.02434 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: 60 pages, 42 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\IWD9BRFM\\1701.html:text/html;Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\YPJ9AVGN\\Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf},
}

@article{hastings_monte_1970,
	title = {Monte {Carlo} {Sampling} {Methods} {Using} {Markov} {Chains} and {Their} {Applications}},
	volume = {57},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334940},
	doi = {10.2307/2334940},
	abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
	number = {1},
	urldate = {2023-08-06},
	journal = {Biometrika},
	author = {Hastings, W. K.},
	year = {1970},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {97--109},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\ZPSR8HQJ\\Hastings - 1970 - Monte Carlo Sampling Methods Using Markov Chains a.pdf:application/pdf},
}

@article{geman_stochastic_1984,
	title = {Stochastic relaxation, gibbs distributions, and the bayesian restoration of images},
	volume = {6},
	issn = {0162-8828},
	doi = {10.1109/tpami.1984.4767596},
	abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
	language = {eng},
	number = {6},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Geman, S. and Geman, D.},
	month = jun,
	year = {1984},
	pmid = {22499653},
	pages = {721--741},
}

@article{casella_explaining_1992,
	title = {Explaining the {Gibbs} {Sampler}},
	volume = {46},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2685208},
	doi = {10.2307/2685208},
	abstract = {Computer-intensive algorithms, such as the Gibbs sampler, have become increasingly popular statistical tools, both in applied and theoretical work. The properties of such algorithms, however, may sometimes not be obvious. Here we give a simple explanation of how and why the Gibbs sampler works. We analytically establish its properties in a simple case and provide insight for more complicated cases. There are also a number of examples.},
	number = {3},
	urldate = {2023-08-06},
	journal = {The American Statistician},
	author = {Casella, George and George, Edward I.},
	year = {1992},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {167--174},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\XBJMB3QZ\\Casella and George - 1992 - Explaining the Gibbs Sampler.pdf:application/pdf},
}

@article{metropolis_equation_2004,
	title = {Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
	volume = {21},
	issn = {0021-9606},
	url = {https://doi.org/10.1063/1.1699114},
	doi = {10.1063/1.1699114},
	abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
	number = {6},
	urldate = {2023-08-06},
	journal = {The Journal of Chemical Physics},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
	month = dec,
	year = {2004},
	pages = {1087--1092},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\2HEQAAU3\\Metropolis et al. - 2004 - Equation of State Calculations by Fast Computing M.pdf:application/pdf;Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\HBTEUFRT\\Equation-of-State-Calculations-by-Fast-Computing.html:text/html},
}

@book{neal_mcmc_2011,
	title = {{MCMC} using {Hamiltonian} dynamics},
	url = {http://arxiv.org/abs/1206.1901},
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
	urldate = {2023-08-06},
	author = {Neal, Radford M.},
	month = may,
	year = {2011},
	doi = {10.1201/b10905},
	note = {arXiv:1206.1901 [physics, stat]},
	keywords = {Physics - Computational Physics, Statistics - Computation},
	file = {arXiv.org Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\AV6YCJFE\\1206.html:text/html;Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\Q2DZQHAA\\Neal - 2011 - MCMC using Hamiltonian dynamics.pdf:application/pdf},
}

@article{homan_no-u-turn_2014,
	title = {The {No}-{U}-turn sampler: adaptively setting path lengths in {Hamiltonian} {Monte} {Carlo}},
	volume = {15},
	issn = {1532-4435},
	shorttitle = {The {No}-{U}-turn sampler},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size ε and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more effciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter ε on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Homan, Matthew D. and Gelman, Andrew},
	month = jan,
	year = {2014},
	keywords = {adaptive Monte Carlo, Bayesian inference, dual averaging, Hamiltonian Monte Carlo, Markov chain Monte Carlo},
	pages = {1593--1623},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\TDQB9C99\\Homan and Gelman - 2014 - The No-U-turn sampler adaptively setting path len.pdf:application/pdf},
}

@misc{betancourt_diagnosing_2016,
	title = {Diagnosing {Suboptimal} {Cotangent} {Disintegrations} in {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1604.00695},
	doi = {10.48550/arXiv.1604.00695},
	abstract = {When properly tuned, Hamiltonian Monte Carlo scales to some of the most challenging high-dimensional problems at the frontiers of applied statistics, but when that tuning is suboptimal the performance leaves much to be desired. In this paper I show how suboptimal choices of one critical degree of freedom, the cotangent disintegration, manifest in readily observed diagnostics that facilitate the robust application of the algorithm.},
	urldate = {2023-08-06},
	publisher = {arXiv},
	author = {Betancourt, Michael},
	month = apr,
	year = {2016},
	note = {arXiv:1604.00695 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: 17 pages, 9 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\JakeL\\Zotero\\storage\\UJLSTZU3\\Betancourt - 2016 - Diagnosing Suboptimal Cotangent Disintegrations in.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\IRCLR3I2\\1604.html:text/html},
}

@misc{betancourt_identity_2020,
	title = {Identity {Crisis}},
	url = {https://betanalpha.github.io/assets/case_studies/identifiability.html},
	abstract = {Under ideal conditions only a small neighborhood of model configurations will be consistent with both the observed data and the domain expertise we encode in our prior model, resulting in a posterior distribution that strongly concentrates along each parameter. This not only yields precise inferences and but also facilitates accurate estimation of those inferences. Under more realistic conditions, however, our measurements and domain expertise can be much less informative, allowing our posterior distribution to stretch across more expansive, complex neighborhoods of the model configuration space. These intricate uncertainties complicate not only the utility of our inferences but also our ability to quantify those inferences computationally.},
	urldate = {2023-08-06},
	author = {Betancourt, Michael},
	month = jun,
	year = {2020},
	file = {Identity Crisis:C\:\\Users\\JakeL\\Zotero\\storage\\FLVNDE89\\identifiability.html:text/html},
}

@article{vehtari_practical_2017-1,
	title = {Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}},
	volume = {27},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-016-9696-4},
	doi = {10.1007/s11222-016-9696-4},
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {Statistics and Computing},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	month = sep,
	year = {2017},
	keywords = {Bayesian computation, K-fold cross-validation, Leave-one-out cross-validation (LOO), Pareto smoothed importance sampling (PSIS), Stan, Widely applicable information criterion (WAIC)},
	pages = {1413--1432},
	file = {Submitted Version:C\:\\Users\\JakeL\\Zotero\\storage\\UDH8698V\\Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf:application/pdf},
}

@misc{vehtari_pareto_2022,
	title = {Pareto {Smoothed} {Importance} {Sampling}},
	url = {http://arxiv.org/abs/1507.02646},
	doi = {10.48550/arXiv.1507.02646},
	abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be highly variable when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates, and convergence diagnostics. The presented Pareto \${\textbackslash}hat\{k\}\$ finite sample convergence rate diagnostic is useful for any Monte Carlo estimator.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
	month = aug,
	year = {2022},
	note = {arXiv:1507.02646 [stat]},
	keywords = {Statistics - Methodology, Statistics - Computation, Statistics - Machine Learning},
	annote = {Comment: Major revision with new additional diagnostics and theory. 50 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\JakeL\\Zotero\\storage\\YT2JLHIT\\Vehtari et al. - 2022 - Pareto Smoothed Importance Sampling.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\SU6TPRW4\\1507.html:text/html},
}

@misc{watanabe_asymptotic_2010,
	title = {Asymptotic {Equivalence} of {Bayes} {Cross} {Validation} and {Widely} {Applicable} {Information} {Criterion} in {Singular} {Learning} {Theory}},
	url = {http://arxiv.org/abs/1004.2316},
	doi = {10.48550/arXiv.1004.2316},
	abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to \$2{\textbackslash}lambda/n\$, where \${\textbackslash}lambda\$ is the real log canonical threshold and \$n\$ is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Watanabe, Sumio},
	month = oct,
	year = {2010},
	note = {arXiv:1004.2316 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\JakeL\\Zotero\\storage\\MDHG7GG9\\Watanabe - 2010 - Asymptotic Equivalence of Bayes Cross Validation a.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\3FBR6FSL\\1004.html:text/html},
}

@article{watanabe_widely_2012,
	title = {A {Widely} {Applicable} {Bayesian} {Information} {Criterion}},
	abstract = {A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive deﬁnite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is deﬁned by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold.},
	language = {en},
	author = {Watanabe, Sumio},
	month = aug,
	year = {2012},
	file = {Watanabe - A Widely Applicable Bayesian Information Criterion.pdf:C\:\\Users\\JakeL\\Zotero\\storage\\E5T2TJUV\\Watanabe - A Widely Applicable Bayesian Information Criterion.pdf:application/pdf},
}

@misc{gelman_understanding_2013,
	title = {Understanding predictive information criteria for {Bayesian} models},
	url = {http://arxiv.org/abs/1307.5928},
	abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
	month = jul,
	year = {2013},
	note = {arXiv:1307.5928 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\RRUCSMFW\\1307.html:text/html;Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\PNIK98TV\\Gelman et al. - 2013 - Understanding predictive information criteria for .pdf:application/pdf},
}

@misc{vehtari_pareto_2022-1,
	title = {Pareto {Smoothed} {Importance} {Sampling}},
	url = {http://arxiv.org/abs/1507.02646},
	abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be highly variable when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates, and convergence diagnostics. The presented Pareto \${\textbackslash}hat\{k\}\$ finite sample convergence rate diagnostic is useful for any Monte Carlo estimator.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
	month = aug,
	year = {2022},
	note = {arXiv:1507.02646 [stat]},
	keywords = {Statistics - Methodology, Statistics - Computation, Statistics - Machine Learning},
	annote = {Comment: Major revision with new additional diagnostics and theory. 50 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\VA2284DE\\1507.html:text/html;Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\EFF9AH3T\\Vehtari et al. - 2022 - Pareto Smoothed Importance Sampling.pdf:application/pdf},
}

@book{mcelreath_statistical_2020,
	title = {Statistical {Rethinking}: {A} {Bayesian} {Course} with {Examples} in {R} and {STAN}},
	isbn = {978-0-429-64231-9},
	shorttitle = {Statistical {Rethinking}},
	abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
	language = {en},
	publisher = {CRC Press},
	author = {McElreath, Richard},
	month = mar,
	year = {2020},
	note = {Google-Books-ID: FuLWDwAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / General, Science / Life Sciences / Biological Diversity},
}

@article{plummer_jags_2003,
	title = {{JAGS}: {A} program for analysis of {Bayesian} graphical models using {Gibbs} sampling},
	abstract = {JAGS is a program for Bayesian Graphical modelling which aims for compatibility with Classic BUGS. The program could eventually be developed as an R package. This article explains the motivations for this program, brieﬂy describes the architecture and then discusses some ideas for a vectorized form of the BUGS language.},
	language = {en},
	journal = {Working Papers},
	author = {Plummer, Martyn},
	year = {2003},
	file = {Plummer - 2003 - JAGS A program for analysis of Bayesian graphical.pdf:C\:\\Users\\JakeL\\Zotero\\storage\\VF7QDIGU\\Plummer - 2003 - JAGS A program for analysis of Bayesian graphical.pdf:application/pdf},
}

@article{merkle_bayesian_2019,
	title = {Bayesian comparison of latent variable models: {Conditional} vs marginal likelihoods},
	volume = {84},
	issn = {0033-3123, 1860-0980},
	shorttitle = {Bayesian comparison of latent variable models},
	url = {http://arxiv.org/abs/1802.04452},
	doi = {10.1007/s11336-019-09679-0},
	abstract = {Typical Bayesian methods for models with latent variables (or random effects) involve directly sampling the latent variables along with the model parameters. In high-level software code for model definitions (using, e.g., BUGS, JAGS, Stan), the likelihood is therefore specified as conditional on the latent variables. This can lead researchers to perform model comparisons via conditional likelihoods, where the latent variables are considered model parameters. In other settings, however, typical model comparisons involve marginal likelihoods where the latent variables are integrated out. This distinction is often overlooked despite the fact that it can have a large impact on the comparisons of interest. In this paper, we clarify and illustrate these issues, focusing on the comparison of conditional and marginal Deviance Information Criteria (DICs) and Watanabe-Akaike Information Criteria (WAICs) in psychometric modeling. The conditional/marginal distinction corresponds to whether the model should be predictive for the clusters that are in the data or for new clusters (where "clusters" typically correspond to higher-level units like people or schools). Correspondingly, we show that marginal WAIC corresponds to leave-one-cluster out (LOcO) cross-validation, whereas conditional WAIC corresponds to leave-one-unit out (LOuO). These results lead to recommendations on the general application of the criteria to models with latent variables.},
	number = {3},
	urldate = {2023-08-10},
	journal = {Psychometrika},
	author = {Merkle, E. C. and Furr, D. and Rabe-Hesketh, S.},
	month = sep,
	year = {2019},
	note = {arXiv:1802.04452 [stat]},
	keywords = {Statistics - Computation},
	pages = {802--829},
	annote = {Comment: Manuscript in press at Psychometrika; 31 pages, 8 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\52T3XB7V\\1802.html:text/html;Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\7IIRRKIP\\Merkle et al. - 2019 - Bayesian comparison of latent variable models Con.pdf:application/pdf},
}

@misc{vehtari_when_2018,
	title = {When {LOO} and other cross-validation approaches are valid},
	url = {https://statmodeling.stat.columbia.edu/2018/08/03/loo-cross-validation-approaches-valid/},
	author = {Vehtari, Aki},
	month = aug,
	year = {2018},
}

@article{cormack_estimates_1964,
	title = {Estimates of survival from the sighting of marked animals},
	volume = {51},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/51.3-4.429},
	doi = {10.1093/biomet/51.3-4.429},
	number = {3-4},
	urldate = {2023-08-10},
	journal = {Biometrika},
	author = {CORMACK, R. M.},
	month = dec,
	year = {1964},
	pages = {429--438},
	file = {Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\KK5BLMKJ\\291928.html:text/html},
}

@article{jolly_explicit_1965,
	title = {Explicit estimates from capture-recapture data with both death and immigration-stochastic model},
	volume = {52},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/52.1-2.225},
	doi = {10.1093/biomet/52.1-2.225},
	number = {1-2},
	urldate = {2023-08-10},
	journal = {Biometrika},
	author = {Jolly, G. M.},
	month = jun,
	year = {1965},
	pages = {225--248},
	file = {Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\QSGBGSFT\\359465.html:text/html},
}

@article{seber_note_1965,
	title = {A note on the multiple-recapture census},
	volume = {52},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/52.1-2.249},
	doi = {10.1093/biomet/52.1-2.249},
	number = {1-2},
	urldate = {2023-08-10},
	journal = {Biometrika},
	author = {Seber, G. A. F.},
	month = jun,
	year = {1965},
	pages = {249--260},
	file = {Full Text PDF:C\:\\Users\\JakeL\\Zotero\\storage\\D7URNZUM\\Seber - 1965 - A note on the multiple-recapture census.pdf:application/pdf;Snapshot:C\:\\Users\\JakeL\\Zotero\\storage\\UF2NTPAC\\359470.html:text/html},
}

@book{jaynes_probability_2002,
	title = {Probability {Theory}: {The} {Logic} of {Science}},
	shorttitle = {Probability {Theory}},
	abstract = {The standard rules of probability can be interpreted as uniquely valid principles in logic. In this book, E. T. Jaynes dispels the imaginary distinction between 'probability theory' and 'statistical inference', leaving a logical unity and simplicity, which provides greater technical power and flexibility in applications. This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. New results are discussed, along with applications of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book will be of interest to scientists working in any area where inference from incomplete information is necessary.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Jaynes, Edwin T.},
	year = {2002},
	note = {Google-Books-ID: ZWVvkQEACAAJ},
}

@book{matsuura_bayesian_2023,
	title = {Bayesian {Statistical} {Modeling} with {Stan}, {R}, and {Python}},
	isbn = {978-981-19475-5-1},
	abstract = {This book provides a highly practical introduction to Bayesian statistical modeling with Stan, which has become the most popular probabilistic programming language.The book is divided into four parts. The first part reviews the theoretical background of modeling and Bayesian inference and presents a modeling workflow that makes modeling more engineering than art. The second part discusses the use of Stan, CmdStanR, and CmdStanPy from the very beginning to basic regression analyses. The third part then introduces a number of probability distributions, nonlinear models, and hierarchical (multilevel) models, which are essential to mastering statistical modeling. It also describes a wide range of frequently used modeling techniques, such as censoring, outliers, missing data, speed-up, and parameter constraints, and discusses how to lead convergence of MCMC. Lastly, the fourth part examines advanced topics for real-world data: longitudinal data analysis, state space models, spatial data analysis, Gaussian processes, Bayesian optimization, dimensionality reduction, model selection, and information criteria, demonstrating that Stan can solve any one of these problems in as little as 30 lines.Using numerous easy-to-understand examples, the book explains key concepts, which continue to be useful when using future versions of Stan and when using other statistical modeling tools. The examples do not require domain knowledge and can be generalized to many fields. The book presents full explanations of code and math formulas, enabling readers to extend models for their own problems. All the code and data are on GitHub.},
	language = {en},
	publisher = {Springer Nature},
	author = {Matsuura, Kentaro},
	month = jan,
	year = {2023},
	note = {Google-Books-ID: eRupEAAAQBAJ},
	keywords = {Business \& Economics / Statistics, Computers / Mathematical \& Statistical Software, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Medical / Biostatistics, Social Science / Research, Social Science / Statistics},
}
